{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM, Dense, TimeDistributed, Embedding, Bidirectional\n",
    "from keras.models import Model, Input\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ner_dataset.csv', encoding = \"ISO-8859-1\")\n",
    "df = df.fillna(method = 'ffill')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a class te get sentence. The each sentence will be list of tuples with its tag and pos.\n",
    "class sentence(object):\n",
    "    def __init__(self, df):\n",
    "        self.n_sent = 1\n",
    "        self.df = df\n",
    "        self.empty = False\n",
    "        agg = lambda s : [(w, p, t) for w, p, t in zip(s['Word'].values.tolist(),\n",
    "                                                       s['POS'].values.tolist(),\n",
    "                                                       s['Tag'].values.tolist())]\n",
    "        self.grouped = self.df.groupby(\"Sentence #\").apply(agg)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_text(self):\n",
    "        try:\n",
    "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
    "            self.n_sent +=1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "#Displaying one full sentence\n",
    "getter = sentence(df)\n",
    "sentences = [\" \".join([s[0] for s in sent]) for sent in getter.sentences]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('Thousands', 'NNS', 'O'), ('of', 'IN', 'O'), ('demonstrators', 'NNS', 'O'), ('have', 'VBP', 'O'), ('marched', 'VBN', 'O'), ('through', 'IN', 'O'), ('London', 'NNP', 'B-geo'), ('to', 'TO', 'O'), ('protest', 'VB', 'O'), ('the', 'DT', 'O'), ('war', 'NN', 'O'), ('in', 'IN', 'O'), ('Iraq', 'NNP', 'B-geo'), ('and', 'CC', 'O'), ('demand', 'VB', 'O'), ('the', 'DT', 'O'), ('withdrawal', 'NN', 'O'), ('of', 'IN', 'O'), ('British', 'JJ', 'B-gpe'), ('troops', 'NNS', 'O'), ('from', 'IN', 'O'), ('that', 'DT', 'O'), ('country', 'NN', 'O'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "#sentence with its pos and tag.\n",
    "sent = getter.get_text()\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of data points passed in each iteration\n",
    "batch_size = 64 \n",
    "# Passes through entire dataset\n",
    "epochs = 8\n",
    "# Maximum length of review\n",
    "max_len = 75 \n",
    "# Dimension of embedding vector\n",
    "embedding = 40 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(df['Word'].unique())\n",
    "tags = list(df['Tag'].unique())\n",
    "\n",
    "word_to_index = { w:i+2 for i,w in enumerate(words)}\n",
    "word_to_index[\"PAD\"] = 1\n",
    "word_to_index[\"UNK\"] = 1\n",
    "\n",
    "tag_to_index = {t:i+1 for i,t in enumerate(tags)}\n",
    "# necssarily force PAD as Zero index.\n",
    "tag_to_index[\"PAD\"] = 0\n",
    "\n",
    "idx2word = {i:w for w,i in word_to_index.items()}\n",
    "idx2tag = {i:t for t,i in tag_to_index.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0]\n",
    "\n",
    "X = [ [word_to_index[w] for w,p,t in s] for s in sentences]\n",
    "X = pad_sequences(sequences=X, maxlen=max_len, padding='post', value=word_to_index['PAD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [ [ tag_to_index[t] for w,p,t in s] for s in sentences ]\n",
    "y = pad_sequences(sequences=y, maxlen=max_len, padding='post', value=tag_to_index['PAD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [3, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 8, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 2, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 6, 7, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tag = df['Tag'].nunique()\n",
    "y = [ to_categorical(i, num_classes=num_tag+1) for i in y ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of training input data :  (40765, 75)\nSize of training output data :  (40765, 75, 18)\nSize of testing input data :  (7194, 75)\nSize of testing output data :  (7194, 75, 18)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Size of training input data : \", X_train.shape)\n",
    "print(\"Size of training output data : \", np.array(y_train).shape)\n",
    "print(\"Size of testing input data : \", X_test.shape)\n",
    "print(\"Size of testing output data : \", np.array(y_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*****Before Processing first sentence : *****\n Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n*****After Processing first sentence : *****\n  [ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 11 17  3 18 19 20 21 22 23\n  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "# Let's check the first sentence before and after processing.\n",
    "print('*****Before Processing first sentence : *****\\n', ' '.join([w[0] for w in sentences[0]]))\n",
    "print('*****After Processing first sentence : *****\\n ', X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*****Before Processing first sentence : *****\n O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n*****After Processing first sentence : *****\n  [[0. 1. 0. ... 0. 0. 0.]\n [0. 1. 0. ... 0. 0. 0.]\n [0. 1. 0. ... 0. 0. 0.]\n ...\n [1. 0. 0. ... 0. 0. 0.]\n [1. 0. 0. ... 0. 0. 0.]\n [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# First label before and after processing.\n",
    "print('*****Before Processing first sentence : *****\\n', ' '.join([w[2] for w in sentences[0]]))\n",
    "print('*****After Processing first sentence : *****\\n ', y[0])"
   ]
  },
  {
   "source": [
    "### Bidirectional LSTM-CRF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "35178\n75\n40\n"
     ]
    }
   ],
   "source": [
    "print(len(words))\n",
    "print(max_len)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1407200"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "35180*40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_4 (InputLayer)         [(None, 10, 128, 128, 3)] 0         \n_________________________________________________________________\ntime_distributed_3 (TimeDist (None, 10, 126, 126, 64)  1792      \n=================================================================\nTotal params: 1,792\nTrainable params: 1,792\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf_1\n",
    "# inputs_1 = tf_1.keras.Input(shape=(10, 128, 128, 3))\n",
    "# conv_2d_layer = tf_1.keras.layers.Conv2D(64, (3, 3))\n",
    "# outputs_1 = tf_1.keras.layers.TimeDistributed(conv_2d_layer)(inputs_1)\n",
    "# outputs_1.shape\n",
    "# mdl_1 = Model(inputs_1,outputs_1)\n",
    "# mdl_1.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "# mdl_1.summary()\n",
    "# # TensorShape([None, 10, 126, 126, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is the inpupt tensor initializer\n",
    "input = Input(shape=(max_len,))\n",
    "\n",
    "# input to Embedding layer is OHE of vocabulary. Output\n",
    "# num_words + PAD + UNK;\n",
    "# 1407200 => (35178+2)*40\n",
    "model = Embedding(input_dim=len(words) + 2, output_dim=embedding, input_length=max_len, mask_zero=True, embeddings_initializer=None)(input)\n",
    "\n",
    "'''\n",
    "stateful: Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "50 hidden units -> \n",
    "Refer Wikipedia for equations ->\n",
    "LSTM Left-to-Right ->\n",
    "50*40 * 4(Wi,Wf, Wo, Wc) = 8,000\n",
    "50*50(Ui,Uf,Uo,Uc) = 10,000\n",
    "50 (bi,bo,bf,bc) -> Bias\n",
    "Total L-to-R -> 18200\n",
    "Total R-to-L -> 18200\n",
    "Total = 2*18200 = 36400 \n",
    "\"\"\"\n",
    "model = Bidirectional( LSTM(50, use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', return_sequences=True) )(model)\n",
    "\n",
    "\"\"\" \n",
    "Hidden unit dimension = 50(left) + 50(right) = 100.\n",
    "TimeDistr preserves history of 50 hidden units. Thus, 50*100 = approx 5000 params\n",
    "\"\"\"\n",
    "model = TimeDistributed(Dense(50, activation='relu'))(model)\n",
    "crf = CRF(num_tag+1)\n",
    "out = crf(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_6\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_9 (InputLayer)         [(None, 75)]              0         \n_________________________________________________________________\nembedding_5 (Embedding)      (None, 75, 40)            1407200   \n_________________________________________________________________\nbidirectional_5 (Bidirection (None, 75, 100)           36400     \n_________________________________________________________________\ntime_distributed_8 (TimeDist (None, 75, 50)            5050      \n_________________________________________________________________\ncrf_3 (CRF)                  (None, 75, 18)            1278      \n=================================================================\nTotal params: 1,449,928\nTrainable params: 1,449,928\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "model = Model(input, out)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "print(model.summary())\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "Making Checkpoint each epoch to check and save the best model performance till last and also avoiding further validation loss drop due to overfitting."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkptr = ModelCheckpoint(\"./model.h5\", monitor='val_acc', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, nop.array(y_train), batch_size=batch_size, )"
   ]
  }
 ]
}